{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flush-submission",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# url = \"https://www.naver.com\"   # 정상적 url 요청\n",
    "url = \"https://www.test.com\"   # 비정상적 url 요청\n",
    "rs = requests.post(url)\n",
    "\n",
    "rs_code = rs.status_code\n",
    "\n",
    "if int(rs_code) == 200:\n",
    "    print(\"페이지 데이터 정상 수신\")\n",
    "    page_data = rs.text\n",
    "    print(page_data)\n",
    "else:\n",
    "    print(rs_code,\"페이지 데이터 수신 실패\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-canal",
   "metadata": {},
   "source": [
    ">>> http response error code 구글검색 정리\n",
    "rs_code : 모든 오류내역을 dictionary 로 해서 프로그램에서 되돌려 줘야함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exterior-start",
   "metadata": {},
   "source": [
    "#### urllib\n",
    "python의 표준모듈로 설치가 필요없음.\n",
    "\n",
    "* API관련 reference : API모음: https://docs.python.org/ko/3.10/library/urllib.request.html\n",
    "\n",
    "robotparser: robots.txt \n",
    ": 검색로봇이 검색 allow하지 못하도록 하기 위해서 기록, 특정폴더 허용하지 않음.\n",
    "법적 문제야기를 막기위해 새로운 웹사이트는 robot.txt를 확인."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-shelf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import requests\n",
    "URL = 'https://introsjlee.pythonanywhere.com'\n",
    "response_requests = requests.get(URL)\n",
    "response_urllib = urllib.request.Request(URL)\n",
    "\n",
    "print(type(response_requests))\n",
    "print(type(response_urllib))\n",
    "\n",
    "print(response_urllib.full_url)\n",
    "print(response_urllib.type)\n",
    "print(response_urllib.host)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-applicant",
   "metadata": {},
   "source": [
    "urllib의 request는 여러가지 method들을 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-insider",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "URL = 'https://introsjlee.pythonanywhere.com'\n",
    "request = urllib.request.Request(URL)\n",
    "\n",
    "response1 = urllib.request.urlopen(request)\n",
    "response2 = urllib.request.urlopen(URL)\n",
    "print(response1)\n",
    "print(response2)\n",
    "print(response1.geturl())\n",
    "print(response1.getheaders())\n",
    "print(response2.geturl())\n",
    "print(response2.getheaders())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-closing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "parse = urllib.parse.urlparse('https://introsjlee.pythonanywhere.com/friends/detail/1')\n",
    "print(parse)\n",
    "print(parse[0])\n",
    "print(parse[1])\n",
    "print(parse[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-potential",
   "metadata": {},
   "source": [
    "urlopen(url[,data [,timeout]])  // timeout은 일정시간이 경과되면 stop\n",
    "\\n 까지가 한줄로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patent-universe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "url = 'https://introsjlee.pythonanywhere.com'\n",
    "response = urllib.request.urlopen(url)\n",
    "byte_data = response.read()\n",
    "\n",
    "print(byte_data)\n",
    "print(response.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-diabetes",
   "metadata": {},
   "source": [
    "- decode : binary 형식을 code형식으로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "URL = 'https://introsjlee.pythonanywhere.com'\n",
    "response = urllib.request.urlopen(URL)\n",
    "byte_data = response.read()\n",
    "text_data = byte_data.decode()\n",
    "print(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmful-wisdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "img_src = 'https://img1.daumcdn.net/relay/cafe/original/?fname=http%3A%2F%2Fcfs8.blog.daum.net%2Fimage%2F33%2Fblog%2F2008%2F08%2F28%2F07%2F02%2F48b5cecb1de78%26filename%3D4%25EC%2595%2584%25EB%25A6%2584%25EB%258B%25A4%25EC%259A%25B4%25ED%2592%258D%25EA%25B2%25BD.jpg'\n",
    "save_name = 'test.png'\n",
    "urllib.request.urlretrieve(img_src, save_name)\n",
    "print(\"저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-environment",
   "metadata": {},
   "source": [
    "rulretrieve (소스경로, 이름설정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-rescue",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "parse = urllib.parse.urlparse('https://introsjlee.pythonanywhere.com/friends/detail/1')\n",
    "print(parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-chile",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "parse = urllib.parse.urlparse('https:/en.dict.naver.com/#/search?query=web')\n",
    "print(parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-butterfly",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "parse = urllib.parse.urlparse('https://finance.naver.com/item/main.nhn?code=005930')\n",
    "print(parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-lawyer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-senegal",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse = list(parse)\n",
    "parse[1] = 'blog.daum.net'\n",
    "\n",
    "unparse = urllib.parse.urlunparse(parse)\n",
    "print(unparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-export",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "parse = urllib.parse.urlparse('https://www.naver.com?a=1&b=2&c=3&d=4')\n",
    "print(parse)\n",
    "print(parse.query)\n",
    "print(type(parse.query))\n",
    "#\n",
    "# # \n",
    "qs = urllib.parse.parse_qs(parse.query)\n",
    "print(qs)\n",
    "print(type(qs))\n",
    "#\n",
    "#\n",
    "qsl = urllib.parse.parse_qsl(parse.query)\n",
    "print(qsl)\n",
    "print(type(qsl))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-present",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contrary-schema",
   "metadata": {},
   "outputs": [],
   "source": [
    "# urljoin(a,b)\n",
    "# a와 b url을 합쳐주는 기능\n",
    "# /에 따라 url주소가 달라지는 것 주의\n",
    "import urllib.parse\n",
    "url = 'https://naver.com/a/b'\n",
    "print(urllib.parse.urljoin(url, 'c'))   # slash전까지 교체\n",
    "print(urllib.parse.urljoin(url, '/c'))  # network location을 초기화/대체함 (path조작)\n",
    "\n",
    "##\n",
    "url = 'https://naver.com/a/b/'\n",
    "print(urllib.parse.urljoin(url, 'c'))\n",
    "print(urllib.parse.urljoin(url, '/c'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-filing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "# url = 'https://search.naver.com/search.naver?query=파이썬'  # 한글이라서...\n",
    "url = 'https://search.naver.com/search.naver?query=' + urllib.parse.quote('파이썬')\n",
    "response = urllib.request.urlopen(url)  # UnicodeEncodeError 발생함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-arrest",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "# url = 'https://search.naver.com/search.naver?query=파이썬'\n",
    "# response = urllib.request.urlopen(url)  # UnicodeEncodeError 발생함\n",
    "encoded = urllib.parse.quote('파이썬')\n",
    "url = 'https://search.naver.com/search.naver?query=' + urllib.parse.quote('파이썬')\n",
    "response = urllib.request.urlopen(url)  # UnicodeEncodeError 발생함\n",
    "byte_data = response.read()\n",
    "text_data = byte_data.decode()\n",
    "print(text_data)  \n",
    "#print(urllib.parse.quote('파이썬'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-mailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "header = {'User-Agent' : 'Mozilla/5.0 (iPhone)'}\n",
    "request = urllib.request.Request(\"http://www.naver.com\", headers=header)\n",
    "data = urllib.request.urlopen(request).read()\n",
    "f = open(\"mobile.html\", \"wb\")\n",
    "f.write(data)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-observer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "query_list = ['파이썬', '웹 크롤링', '빅데이터', 'python']\n",
    "url = 'https://search.naver.com/search.naver?query='\n",
    "for i in query_list:\n",
    "    new_url = url + urllib.parse.quote(i)\n",
    "    response = urllib.request.urlopen(new_url)\n",
    "    byte_data = response.read()\n",
    "    text_data = byte_data.decode()\n",
    "    print(text_data)\n",
    "    print(\"\\n\")\n",
    "    print(\"=========================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-mouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-february",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-while",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "\n",
    "URL = 'http://www.naver.com'\n",
    "response_urllib = urllib.request.urlopen(URL)\n",
    "byte_data = response_urllib.read()\n",
    "text_data = byte_data.decode()\n",
    "\n",
    "soup = BeautifulSoup(text_data, 'html.parser')\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-forty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "req = requests.get('https://naver.com')\n",
    "html = req.text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "result = soup.find_all('a', 'thumb', limit=5)\n",
    "news_list = []\n",
    "for i in result:\n",
    "    news_list.append(i.find(\"img\")[\"alt\"])\n",
    "print(news_list)\n",
    "\n",
    "print(soup.title)\n",
    "print(soup.title.name)\n",
    "print(soup.title.string)\n",
    "\n",
    "print(soup.img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-batman",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "req = requests.get('https://naver.com')\n",
    "html = req.text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "result = soup.find_all('a', 'thumb', limit=5)\n",
    "news_list = []\n",
    "for i in result:\n",
    "    news_list.append(i.find(\"img\")[\"src\"])\n",
    "print(news_list)\n",
    "\n",
    "print(soup.title)\n",
    "print(soup.title.name)\n",
    "print(soup.title.string)\n",
    "\n",
    "print(soup.img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-contact",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이버 영화 랭킹 가져오기\n",
    "# 1. 홈페이지 텍스트 가져오기\n",
    "# https://movie.naver.com/movie/sdb/rank/rmovie.nhn\n",
    "# 2. BeautifulSoup으로 파싱하기\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "URL = \"https://movie.naver.com/movie/sdb/rank/rmovie.nhn\"\n",
    "req = requests.get(URL)\n",
    "html = req.text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-attitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트에서 영화 랭킹 가져오기 Step 2.\n",
    "# 1. 텍스트에서 영화 랭킹 찾기\n",
    "# 2. 영화랭킹에 해당하는 부분의 태그 찾기\n",
    "movie_ranking_list = soup.find_all('div', 'tit3')\n",
    "for i in range(len(movie_ranking_list)):\n",
    "    print(\"{:2}위:{}\".format(i+1, movie_ranking_list[i].get_text().strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-category",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드를 작성하세요.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "URL = \"http://news.naver.com\"\n",
    "req = requests.get(URL)\n",
    "html = req.text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "print(soup)\n",
    "\n",
    "head_news_list = []\n",
    "result_bit = soup.find_all('p', \"hdline_fick_kit\")\n",
    "for i in result_big:\n",
    "    head_news_list.append(i.get_text())\n",
    "result_small = soup.find_all('div', \"hdline_article_tit\")\n",
    "for i in result_small:\n",
    "    head_news_list.append(i.get_text().strip())\n",
    "print(head_news_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
